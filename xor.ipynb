{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xor.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Lzdlx8kld7sN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "def createXorData(samples=100000, size=50):\n",
        "  \"\"\"\n",
        "  generates a dataset for the XOR problem in \n",
        "    https://blog.openai.com/requests-for-research-2/\n",
        "  LHS are sequences of bools\n",
        "  RHS computes the parity bit \n",
        "    (eg. wether the sequence has odd or even sum)\n",
        "  \"\"\"\n",
        "  lhs = np.zeros((samples, size), dtype=np.int8)\n",
        "  rhs = np.zeros(samples)\n",
        "  for row in range(samples):\n",
        "    lhs[row] = np.random.randint(0, 2, size)\n",
        "    rhs[row] = lhs[row].sum() % 2\n",
        "  return lhs, rhs\n",
        "\n",
        "\n",
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels from an array \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wrDYUx8d-b8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "SIZE = 3\n",
        "SAMPLES = 20000\n",
        "LEARNING_RATE = 0.001\n",
        "TRAIN_STEPS = 2500\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "Xtrain, Ytrain = createXorData(SAMPLES, SIZE)\n",
        "Xtest, Ytest = createXorData(SAMPLES//2, SIZE)\n",
        "\n",
        "X_lenghts = np.ones((SAMPLES, 1)) * SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBc-pUFX-oQk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b4bcb440-441e-415b-fd23-cc61afd496fa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521684858508,
          "user_tz": 240,
          "elapsed": 7126,
          "user": {
            "displayName": "Matthieu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103782916960282194779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# overkill MLP\n",
        "# Seems to work up to input length of 22\n",
        "#\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(25, 25,25), \n",
        "                    activation=\"relu\"\n",
        "                   ).fit(Xtrain, Ytrain)\n",
        "pred_score = mlp.score(Xtest, Ytest)\n",
        "print(pred_score)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aliL6nHW-y_g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LSTM(object):\n",
        "    def __init__(self, hidden_units, num_classes=2, \n",
        "                 max_sequence_length=SIZE, \n",
        "                 random_seed=np.random.randint(150)):\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_classes = num_classes\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.random_seed = random_seed\n",
        "        \n",
        "        self.g = tf.Graph()\n",
        "        with self.g.as_default():\n",
        "            self.X = tf.placeholder(tf.float32, \n",
        "                                    (None, self.max_sequence_length, 1), \n",
        "                                    name='sequences')\n",
        "            self.X_len = tf.placeholder(tf.int32,\n",
        "                                        (None, 1),\n",
        "                                        name='lengths')\n",
        "            self.Y = tf.placeholder(tf.int32, \n",
        "                                    (None),\n",
        "                                    name='parity_labels')\n",
        "\n",
        "            batch_dim = tf.shape(self.X)[0]\n",
        "            with tf.variable_scope(\"LSTM\"):\n",
        "                tf.set_random_seed(self.random_seed)\n",
        "                cell = tf.contrib.rnn.LSTMCell(self.hidden_units)\n",
        "                initial_state = cell.zero_state(batch_dim, tf.float32)\n",
        "                outputs, state = tf.nn.dynamic_rnn(cell, \n",
        "                                                   self.X, \n",
        "                                                   initial_state=initial_state, \n",
        "                                                   dtype=tf.float32)\n",
        "            # get last output of rnn\n",
        "            indices = self.X_len[-1] - 1\n",
        "            rnn_out = tf.gather(outputs, indices, axis=1)\n",
        "            self.final_rnn_out = tf.reshape(tf.squeeze(rnn_out),\n",
        "                                            (-1, self.hidden_units))\n",
        "            with tf.variable_scope(\"Linear\"):\n",
        "                W = tf.Variable(tf.random_normal((self.hidden_units, num_classes), \n",
        "                                                  stddev=0.1, \n",
        "                                                 seed=self.random_seed))\n",
        "                b = tf.Variable(tf.zeros((num_classes)))\n",
        "                self.h = tf.add(tf.matmul(self.final_rnn_out, W), b)\n",
        "\n",
        "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.h, labels=self.Y))\n",
        "            self.train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "\n",
        "            self.predictions = tf.reshape(tf.squeeze(\n",
        "                                 tf.argmax(self.h, axis=1)), (-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCFI1Jlz_wb-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, Xtrain, Ytrain, Xtest, Ytest, X_lenghts,\n",
        "                epochs=100, batch_size=BATCH_SIZE, dataset_size=SAMPLES):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    num_batches = dataset_size//batch_size\n",
        "    with model.g.as_default():\n",
        "        sess = tf.Session()\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        for t in range(epochs):\n",
        "            print(\"Epoch {}\\n\".format(t+1))\n",
        "            for i in range(0, num_batches):\n",
        "                X_batch = Xtrain[i:i+batch_size]\n",
        "                Y_batch = Ytrain[i:i+batch_size]\n",
        "                X_len_batch = X_lenghts[i:i+batch_size]\n",
        "                _ = sess.run(model.train_op, \n",
        "                             feed_dict={model.X: X_batch, \n",
        "                                        model.Y: Y_batch, \n",
        "                                        model.X_len: X_len_batch})\n",
        "                if i % 100 == 0:\n",
        "                    loss_ = sess.run(model.loss,\n",
        "                                     feed_dict={model.X: Xtrain, \n",
        "                                                model.Y: Ytrain,\n",
        "                                                model.X_len: X_len_batch})\n",
        "                    pred = sess.run(model.predictions, \n",
        "                                    feed_dict={model.X: Xtest, \n",
        "                                               model.Y: Ytest,\n",
        "                                               model.X_len: X_len_batch})\n",
        "                    accuracy = np.mean(pred == Ytest)\n",
        "                    print('iteration: {}, loss: {},  accuracy: {}'.format(i+1, loss_, accuracy))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7e8yE3fCDUE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 52
            },
            {
              "item_id": 87
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 5677
        },
        "outputId": "97c647ab-db42-4c96-e8aa-7b7a25eb6a03",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521690423308,
          "user_tz": 240,
          "elapsed": 89370,
          "user": {
            "displayName": "Matthieu",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "103782916960282194779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "k_lstm_units = LSTM(SIZE)\n",
        "\n",
        "Xtrain_r = Xtrain.reshape((SAMPLES, SIZE, 1))\n",
        "Xtest_r = Xtest.reshape((SAMPLES//2, SIZE, 1))\n",
        "\n",
        "train_model(k_lstm_units, Xtrain_r, Ytrain, Xtest_r, Ytest, X_lenghts)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\n",
            "iteration: 1, loss: 0.6931548118591309,  accuracy: 0.49781818\n",
            "Epoch 2\n",
            "\n",
            "iteration: 1, loss: 0.6934610605239868,  accuracy: 0.4957\n",
            "Epoch 3\n",
            "\n",
            "iteration: 1, loss: 0.6933797597885132,  accuracy: 0.4957\n",
            "Epoch 4\n",
            "\n",
            "iteration: 1, loss: 0.693368136882782,  accuracy: 0.4957\n",
            "Epoch 5\n",
            "\n",
            "iteration: 1, loss: 0.693374752998352,  accuracy: 0.4957\n",
            "Epoch 6\n",
            "\n",
            "iteration: 1, loss: 0.6933584809303284,  accuracy: 0.49679306\n",
            "Epoch 7\n",
            "\n",
            "iteration: 1, loss: 0.6933428049087524,  accuracy: 0.49679306\n",
            "Epoch 8\n",
            "\n",
            "iteration: 1, loss: 0.6933079957962036,  accuracy: 0.49679306\n",
            "Epoch 9\n",
            "\n",
            "iteration: 1, loss: 0.6932434439659119,  accuracy: 0.49679306\n",
            "Epoch 10\n",
            "\n",
            "iteration: 1, loss: 0.693108856678009,  accuracy: 0.49679306\n",
            "Epoch 11\n",
            "\n",
            "iteration: 1, loss: 0.6928402185440063,  accuracy: 0.49679306\n",
            "Epoch 12\n",
            "\n",
            "iteration: 1, loss: 0.6922440528869629,  accuracy: 0.49679306\n",
            "Epoch 13\n",
            "\n",
            "iteration: 1, loss: 0.6910783052444458,  accuracy: 0.49790504\n",
            "Epoch 14\n",
            "\n",
            "iteration: 1, loss: 0.6888827085494995,  accuracy: 0.49790504\n",
            "Epoch 15\n",
            "\n",
            "iteration: 1, loss: 0.6850487589836121,  accuracy: 0.49790504\n",
            "Epoch 16\n",
            "\n",
            "iteration: 1, loss: 0.678795576095581,  accuracy: 0.49790504\n",
            "Epoch 17\n",
            "\n",
            "iteration: 1, loss: 0.6695271730422974,  accuracy: 0.49679306\n",
            "Epoch 18\n",
            "\n",
            "iteration: 1, loss: 0.657297670841217,  accuracy: 0.49784484\n",
            "Epoch 19\n",
            "\n",
            "iteration: 1, loss: 0.6429013609886169,  accuracy: 0.49895166\n",
            "Epoch 20\n",
            "\n",
            "iteration: 1, loss: 0.6274713277816772,  accuracy: 0.49895166\n",
            "Epoch 21\n",
            "\n",
            "iteration: 1, loss: 0.6119980216026306,  accuracy: 0.49895166\n",
            "Epoch 22\n",
            "\n",
            "iteration: 1, loss: 0.59708571434021,  accuracy: 0.49895166\n",
            "Epoch 23\n",
            "\n",
            "iteration: 1, loss: 0.5828658938407898,  accuracy: 0.49895166\n",
            "Epoch 24\n",
            "\n",
            "iteration: 1, loss: 0.5692088007926941,  accuracy: 0.49895166\n",
            "Epoch 25\n",
            "\n",
            "iteration: 1, loss: 0.5559038519859314,  accuracy: 0.49895166\n",
            "Epoch 26\n",
            "\n",
            "iteration: 1, loss: 0.5428165197372437,  accuracy: 0.49895166\n",
            "Epoch 27\n",
            "\n",
            "iteration: 1, loss: 0.5299230217933655,  accuracy: 0.50002236\n",
            "Epoch 28\n",
            "\n",
            "iteration: 1, loss: 0.5172885656356812,  accuracy: 0.50002236\n",
            "Epoch 29\n",
            "\n",
            "iteration: 1, loss: 0.5049744844436646,  accuracy: 0.50002236\n",
            "Epoch 30\n",
            "\n",
            "iteration: 1, loss: 0.49299073219299316,  accuracy: 0.50108876\n",
            "Epoch 31\n",
            "\n",
            "iteration: 1, loss: 0.4813527464866638,  accuracy: 0.50108876\n",
            "Epoch 32\n",
            "\n",
            "iteration: 1, loss: 0.4700832962989807,  accuracy: 0.50108876\n",
            "Epoch 33\n",
            "\n",
            "iteration: 1, loss: 0.4591555595397949,  accuracy: 0.50108876\n",
            "Epoch 34\n",
            "\n",
            "iteration: 1, loss: 0.44855669140815735,  accuracy: 0.50108876\n",
            "Epoch 35\n",
            "\n",
            "iteration: 1, loss: 0.43827638030052185,  accuracy: 0.50108876\n",
            "Epoch 36\n",
            "\n",
            "iteration: 1, loss: 0.4282829165458679,  accuracy: 0.50108876\n",
            "Epoch 37\n",
            "\n",
            "iteration: 1, loss: 0.41853711009025574,  accuracy: 0.50108876\n",
            "Epoch 38\n",
            "\n",
            "iteration: 1, loss: 0.4090208113193512,  accuracy: 0.50108876\n",
            "Epoch 39\n",
            "\n",
            "iteration: 1, loss: 0.39969998598098755,  accuracy: 0.50108876\n",
            "Epoch 40\n",
            "\n",
            "iteration: 1, loss: 0.390514075756073,  accuracy: 0.50108876\n",
            "Epoch 41\n",
            "\n",
            "iteration: 1, loss: 0.38142940402030945,  accuracy: 0.50108876\n",
            "Epoch 42\n",
            "\n",
            "iteration: 1, loss: 0.3723888397216797,  accuracy: 0.50108876\n",
            "Epoch 43\n",
            "\n",
            "iteration: 1, loss: 0.3633333146572113,  accuracy: 0.50108876\n",
            "Epoch 44\n",
            "\n",
            "iteration: 1, loss: 0.3541906177997589,  accuracy: 0.50108876\n",
            "Epoch 45\n",
            "\n",
            "iteration: 1, loss: 0.34488382935523987,  accuracy: 0.50108876\n",
            "Epoch 46\n",
            "\n",
            "iteration: 1, loss: 0.33532607555389404,  accuracy: 0.50108876\n",
            "Epoch 47\n",
            "\n",
            "iteration: 1, loss: 0.3254471719264984,  accuracy: 0.50108876\n",
            "Epoch 48\n",
            "\n",
            "iteration: 1, loss: 0.31517210602760315,  accuracy: 0.50108876\n",
            "Epoch 49\n",
            "\n",
            "iteration: 1, loss: 0.30445554852485657,  accuracy: 0.50108876\n",
            "Epoch 50\n",
            "\n",
            "iteration: 1, loss: 0.29327723383903503,  accuracy: 0.50108876\n",
            "Epoch 51\n",
            "\n",
            "iteration: 1, loss: 0.2816470265388489,  accuracy: 0.50108876\n",
            "Epoch 52\n",
            "\n",
            "iteration: 1, loss: 0.2696267068386078,  accuracy: 0.50108876\n",
            "Epoch 53\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration: 1, loss: 0.2573140561580658,  accuracy: 0.50108876\n",
            "Epoch 54\n",
            "\n",
            "iteration: 1, loss: 0.24483217298984528,  accuracy: 0.50108876\n",
            "Epoch 55\n",
            "\n",
            "iteration: 1, loss: 0.23232348263263702,  accuracy: 0.50108876\n",
            "Epoch 56\n",
            "\n",
            "iteration: 1, loss: 0.21993906795978546,  accuracy: 0.50003698\n",
            "Epoch 57\n",
            "\n",
            "iteration: 1, loss: 0.20779502391815186,  accuracy: 0.50003698\n",
            "Epoch 58\n",
            "\n",
            "iteration: 1, loss: 0.19600330293178558,  accuracy: 0.50003698\n",
            "Epoch 59\n",
            "\n",
            "iteration: 1, loss: 0.1846395581960678,  accuracy: 0.50003698\n",
            "Epoch 60\n",
            "\n",
            "iteration: 1, loss: 0.17376267910003662,  accuracy: 0.50003698\n",
            "Epoch 61\n",
            "\n",
            "iteration: 1, loss: 0.16340188682079315,  accuracy: 0.50003698\n",
            "Epoch 62\n",
            "\n",
            "iteration: 1, loss: 0.15358573198318481,  accuracy: 0.50003698\n",
            "Epoch 63\n",
            "\n",
            "iteration: 1, loss: 0.14431868493556976,  accuracy: 0.50003698\n",
            "Epoch 64\n",
            "\n",
            "iteration: 1, loss: 0.13560490310192108,  accuracy: 0.50003698\n",
            "Epoch 65\n",
            "\n",
            "iteration: 1, loss: 0.12743954360485077,  accuracy: 0.50003698\n",
            "Epoch 66\n",
            "\n",
            "iteration: 1, loss: 0.11980558931827545,  accuracy: 0.50003698\n",
            "Epoch 67\n",
            "\n",
            "iteration: 1, loss: 0.11268896609544754,  accuracy: 0.50003698\n",
            "Epoch 68\n",
            "\n",
            "iteration: 1, loss: 0.10606370866298676,  accuracy: 0.50003698\n",
            "Epoch 69\n",
            "\n",
            "iteration: 1, loss: 0.09990590065717697,  accuracy: 0.50003698\n",
            "Epoch 70\n",
            "\n",
            "iteration: 1, loss: 0.09418803453445435,  accuracy: 0.50003698\n",
            "Epoch 71\n",
            "\n",
            "iteration: 1, loss: 0.08888045698404312,  accuracy: 0.50003698\n",
            "Epoch 72\n",
            "\n",
            "iteration: 1, loss: 0.08395903557538986,  accuracy: 0.50003698\n",
            "Epoch 73\n",
            "\n",
            "iteration: 1, loss: 0.07939214259386063,  accuracy: 0.50003698\n",
            "Epoch 74\n",
            "\n",
            "iteration: 1, loss: 0.07515556365251541,  accuracy: 0.50003698\n",
            "Epoch 75\n",
            "\n",
            "iteration: 1, loss: 0.07122322171926498,  accuracy: 0.50003698\n",
            "Epoch 76\n",
            "\n",
            "iteration: 1, loss: 0.06757014244794846,  accuracy: 0.50003698\n",
            "Epoch 77\n",
            "\n",
            "iteration: 1, loss: 0.0641758143901825,  accuracy: 0.50003698\n",
            "Epoch 78\n",
            "\n",
            "iteration: 1, loss: 0.06101911514997482,  accuracy: 0.50003698\n",
            "Epoch 79\n",
            "\n",
            "iteration: 1, loss: 0.05808009207248688,  accuracy: 0.50003698\n",
            "Epoch 80\n",
            "\n",
            "iteration: 1, loss: 0.0553404726088047,  accuracy: 0.50003698\n",
            "Epoch 81\n",
            "\n",
            "iteration: 1, loss: 0.05278511345386505,  accuracy: 0.50003698\n",
            "Epoch 82\n",
            "\n",
            "iteration: 1, loss: 0.05039845034480095,  accuracy: 0.50003698\n",
            "Epoch 83\n",
            "\n",
            "iteration: 1, loss: 0.04816744476556778,  accuracy: 0.50003698\n",
            "Epoch 84\n",
            "\n",
            "iteration: 1, loss: 0.04607851058244705,  accuracy: 0.50003698\n",
            "Epoch 85\n",
            "\n",
            "iteration: 1, loss: 0.04412202164530754,  accuracy: 0.50003698\n",
            "Epoch 86\n",
            "\n",
            "iteration: 1, loss: 0.04228632152080536,  accuracy: 0.50003698\n",
            "Epoch 87\n",
            "\n",
            "iteration: 1, loss: 0.040561988949775696,  accuracy: 0.50003698\n",
            "Epoch 88\n",
            "\n",
            "iteration: 1, loss: 0.03894130885601044,  accuracy: 0.50003698\n",
            "Epoch 89\n",
            "\n",
            "iteration: 1, loss: 0.03741580247879028,  accuracy: 0.50003698\n",
            "Epoch 90\n",
            "\n",
            "iteration: 1, loss: 0.03597826510667801,  accuracy: 0.50003698\n",
            "Epoch 91\n",
            "\n",
            "iteration: 1, loss: 0.03462246060371399,  accuracy: 0.50003698\n",
            "Epoch 92\n",
            "\n",
            "iteration: 1, loss: 0.033342473208904266,  accuracy: 0.50003698\n",
            "Epoch 93\n",
            "\n",
            "iteration: 1, loss: 0.03213278949260712,  accuracy: 0.50003698\n",
            "Epoch 94\n",
            "\n",
            "iteration: 1, loss: 0.03098788484930992,  accuracy: 0.50003698\n",
            "Epoch 95\n",
            "\n",
            "iteration: 1, loss: 0.029903888702392578,  accuracy: 0.50003698\n",
            "Epoch 96\n",
            "\n",
            "iteration: 1, loss: 0.02887638472020626,  accuracy: 0.50003698\n",
            "Epoch 97\n",
            "\n",
            "iteration: 1, loss: 0.027901433408260345,  accuracy: 0.50003698\n",
            "Epoch 98\n",
            "\n",
            "iteration: 1, loss: 0.02697576954960823,  accuracy: 0.50003698\n",
            "Epoch 99\n",
            "\n",
            "iteration: 1, loss: 0.026095757260918617,  accuracy: 0.50003698\n",
            "Epoch 100\n",
            "\n",
            "iteration: 1, loss: 0.02525871805846691,  accuracy: 0.50003698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lMwqXa9avJKg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}